{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xq7-PkKGFlG7",
    "outputId": "11b85648-0b89-4cae-bb6c-62b3fc5b9e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cpu)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pygame opencv-python numpy torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZRUSQsB5sb9G"
   },
   "outputs": [],
   "source": [
    "RUNNING_ON_COLAB = False # Use True for colab. It will ask for a google drive permission where you have your assets and weights.\n",
    "                        # Although the visual assets are not needed on colab since the screen is not shown, this was made for convenience when developing this project.\n",
    "                        # Maybe in further updates this will be changed so no assets are needed on google drive.\n",
    "\n",
    "EVALUATION_MODE = True # Use False for training the model, True for testing it\n",
    "\n",
    "PROJECT_ROOT = \"./\" # If you downloaded the repo locally (and running it using Jupyter Lab for example) and your working directory is FlapFlapDQN, you can leave this variable as is.\n",
    "                    # If you downloaded the repo locally (and running it using Jupyter Lab for example) and your working directory is NOT FlapFlapDQN, use an absolute path (or relative paths using ../ if you're that type of guy)\n",
    "                    # If you are running on colab and the project is at the root of My Drive, you should add \"./FlapFlapDQN/\"\n",
    "                    # If you are running on colab and the project is in a subfolder of My Drive, add the part after 'My Drive' as a relative path.\n",
    "                    # Example: if you have your files in My Drive/github_projects/reinforcement_learning/FlapFlapDQN add \"./github_projects/reinforcement_learning/FlapFlapDQN\"\n",
    "\n",
    "PRETRAINED_MODEL_WEIGHTS = \"FlapFlap_725000.pth\" # Change this to None if you want to train it from scratch\n",
    "SAVE_MODEL_EVERY = 25000 # a new .pth file will be created every SAVE_MODEL_EVERY iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxZ8QcwX14VE"
   },
   "source": [
    "### Environment Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXPLyru1FiA4",
    "outputId": "1756c73e-7e83-4897-e9b4-1f56db77043b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pygame\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "if RUNNING_ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = os.path.join('/content/drive/My Drive/', PROJECT_ROOT)\n",
    "\n",
    "def load():\n",
    "    # path of player with different states\n",
    "    PLAYER_PATH = (\n",
    "        os.path.join(PROJECT_ROOT, './assets/sprites/redbird-upflap.png'),\n",
    "        os.path.join(PROJECT_ROOT, './assets/sprites/redbird-midflap.png'),\n",
    "        os.path.join(PROJECT_ROOT, './assets/sprites/redbird-downflap.png')\n",
    "    )\n",
    "\n",
    "    # path of background\n",
    "    BACKGROUND_PATH = os.path.join(PROJECT_ROOT, './assets/sprites/background-black.png')\n",
    "\n",
    "    # path of pipe\n",
    "    PIPE_PATH = os.path.join(PROJECT_ROOT, './assets/sprites/pipe-green.png')\n",
    "\n",
    "    IMAGES, HITMASKS = {}, {}\n",
    "\n",
    "    # numbers sprites for score display\n",
    "    IMAGES['numbers'] = [\n",
    "        pygame.image.load(os.path.join(PROJECT_ROOT, './assets/sprites/{}.png'.format(idx))).convert_alpha()\n",
    "        for idx in range(10)\n",
    "    ]\n",
    "\n",
    "    # base (ground) sprite\n",
    "    IMAGES['base'] = pygame.image.load(os.path.join(PROJECT_ROOT, './assets/sprites/base.png')).convert_alpha()\n",
    "\n",
    "    # select random background sprites\n",
    "    IMAGES['background'] = pygame.image.load(BACKGROUND_PATH).convert()\n",
    "\n",
    "    # select random player sprites\n",
    "    IMAGES['player'] = [\n",
    "        pygame.image.load(PLAYER_PATH[idx]).convert_alpha()\n",
    "        for idx in range(3)\n",
    "    ]\n",
    "\n",
    "    # select random pipe sprites\n",
    "    IMAGES['pipe'] = (\n",
    "        pygame.transform.rotate(\n",
    "            pygame.image.load(PIPE_PATH).convert_alpha(), 180),\n",
    "        pygame.image.load(PIPE_PATH).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # hismask for pipes\n",
    "    HITMASKS['pipe'] = [\n",
    "        getHitmask(IMAGES['pipe'][idx])\n",
    "        for idx in range(2)\n",
    "    ]\n",
    "\n",
    "    # hitmask for player\n",
    "    HITMASKS['player'] = [\n",
    "        getHitmask(IMAGES['player'][idx])\n",
    "        for idx in range(3)\n",
    "    ]\n",
    "\n",
    "    return IMAGES, HITMASKS\n",
    "\n",
    "\n",
    "def getHitmask(image):\n",
    "    \"\"\"returns a hitmask using an image's alpha.\"\"\"\n",
    "    mask = []\n",
    "    for x in range(image.get_width()):\n",
    "        mask.append([])\n",
    "        for y in range(image.get_height()):\n",
    "            mask[x].append(bool(image.get_at((x, y))[3]))\n",
    "    return mask\n",
    "\n",
    "\n",
    "FPS = 240\n",
    "SCREENWIDTH = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "# pygame.init()\n",
    "FPSCLOCK = pygame.time.Clock()\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "pygame.display.set_caption('FlapFlapDQN')\n",
    "\n",
    "IMAGES, HITMASKS = load()\n",
    "PIPEGAPSIZE = 100  # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self, show_game):\n",
    "        self.show_game = show_game\n",
    "\n",
    "        if self.show_game:\n",
    "            pygame.init()\n",
    "            self.SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "            pygame.display.set_caption('FlapFlapDQN')\n",
    "\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # player velocity, max velocity, downward accleration, accleration on flap\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY = 0  # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY = 10  # max vel along Y, max descend speed\n",
    "        self.playerMinVelY = -8  # min vel along Y, max ascend speed\n",
    "        self.playerAccY = 1  # players downward accleration\n",
    "        self.playerFlapAcc = -9  # players speed on flapping\n",
    "        self.playerFlapped = False  # True when player flaps\n",
    "        self.top_score = 0\n",
    "\n",
    "    def frame_step(self, input_actions):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 0.1\n",
    "        terminal = False\n",
    "\n",
    "        if sum(input_actions) != 1:\n",
    "            raise ValueError('Multiple input actions!')\n",
    "\n",
    "        # input_actions[0] == 1: do nothing\n",
    "        # input_actions[1] == 1: flap the bird\n",
    "        if input_actions[1] == 1:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.top_score += 1\n",
    "                self.score += 1\n",
    "                reward = 1\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if its out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash = checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                              'index': self.playerIndex},\n",
    "                             self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            terminal = True\n",
    "            self.__init__(self.show_game)\n",
    "            reward = -1\n",
    "\n",
    "\n",
    "        self.SCREEN.blit(IMAGES['background'], (0, 0))\n",
    "\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            self.SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "            self.SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        self.SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        self.SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "                            (self.playerx, self.playery))\n",
    "\n",
    "        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "\n",
    "        if self.show_game:\n",
    "            showScore(self.top_score)\n",
    "            pygame.display.update()\n",
    "\n",
    "        FPSCLOCK.tick(FPS)\n",
    "        return image_data, reward, terminal\n",
    "\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs) - 1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0  # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                                 player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1 + x][y1 + y] and hitmask2[x2 + x][y2 + y]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO3ZCoZo19aC"
   },
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "O3N4Tgm9FiqD",
    "outputId": "0f74ceec-2628-496f-a8ce-52c459396b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model FlapFlap_725000.pth\n",
      "Current points: 10\n",
      "Current points: 20\n",
      "Current points: 30\n",
      "Current points: 40\n",
      "Current points: 50\n",
      "Current points: 60\n",
      "Current points: 70\n",
      "Current points: 80\n",
      "Current points: 90\n",
      "Current points: 100\n",
      "Current points: 110\n",
      "\n",
      " ------- Finished -------\n",
      "Points: 113 Total reward: 411.40000000002505\n",
      "------------------------\n",
      "Current points: 10\n",
      "Current points: 20\n",
      "Current points: 30\n",
      "Current points: 40\n",
      "Current points: 50\n",
      "Current points: 60\n",
      "Current points: 70\n",
      "Current points: 80\n",
      "Current points: 90\n",
      "Current points: 100\n",
      "Current points: 110\n",
      "Current points: 120\n",
      "Current points: 130\n",
      "\n",
      " ------- Finished -------\n",
      "Points: 131 Total reward: 476.3000000000398\n",
      "------------------------\n",
      "Current points: 10\n",
      "Current points: 20\n",
      "Current points: 30\n",
      "Current points: 40\n",
      "Current points: 50\n",
      "Current points: 60\n",
      "Current points: 70\n",
      "Current points: 80\n",
      "Current points: 90\n",
      "Current points: 100\n",
      "Current points: 110\n",
      "Current points: 120\n",
      "Current points: 130\n",
      "Current points: 140\n",
      "Current points: 150\n",
      "Current points: 160\n",
      "Current points: 170\n",
      "Current points: 180\n",
      "Current points: 190\n",
      "Current points: 200\n",
      "Current points: 210\n",
      "Current points: 220\n",
      "Current points: 230\n",
      "Current points: 240\n",
      "Current points: 250\n",
      "Current points: 260\n",
      "Current points: 270\n",
      "Current points: 280\n",
      "Current points: 290\n",
      "Current points: 300\n",
      "Current points: 310\n",
      "Current points: 320\n",
      "Current points: 330\n"
     ]
    }
   ],
   "source": [
    "if RUNNING_ON_COLAB:\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "\n",
    "class Hyperparameters:\n",
    "    MODEL_NAME = \"FlapFlap\"\n",
    "    LEARNING_RATE = 1e-5\n",
    "    FRAME_SKIP = 1\n",
    "    FRAME_SKIP_JUMP = 0\n",
    "    SHOW_GAME = True\n",
    "    NUMBER_OF_ACTIONS = 2\n",
    "    GAMMA = 0.99\n",
    "    INITIAL_EPSILON = 0.2\n",
    "    FINAL_EPSILON = 0.00001\n",
    "    NUMBER_OF_ITERATIONS = 2_000_000\n",
    "    REPLAY_MEMORY_SIZE = 50_000 # Decrease this if your machine crashes due to lack of RAM. In this configuration, google colab uses ~20 GB of RAM\n",
    "    MINIBATCH_SIZE = 32\n",
    "    TARGET_UPDATE_FREQUENCY = 1000\n",
    "    CUTOFF = 1_000_000 # The iteration when the epsilon will reach FINAL_EPSILON.\n",
    "                       # At this point the network can continue to train forever with a fixed epsilon FINAL_EPSILON\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.fc4 = nn.Linear(3136, 512)\n",
    "        self.fc5 = nn.Linear(512, Hyperparameters.NUMBER_OF_ACTIONS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = self.conv2(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = self.conv3(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc4(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = self.fc5(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "i = 0\n",
    "\n",
    "def image_processing(image):\n",
    "    image = image[:, 40:300]\n",
    "    image_data = cv2.cvtColor(cv2.resize(image, (84, 84)), cv2.COLOR_BGR2GRAY)\n",
    "    image_data[image_data > 0] = 255\n",
    "    image_data = np.reshape(image_data, (84, 84, 1))\n",
    "    image_tensor = image_data.transpose(2, 0, 1)\n",
    "    image_tensor = image_tensor.astype(np.float32)\n",
    "    image_tensor = torch.from_numpy(image_tensor)\n",
    "    if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
    "        image_tensor = image_tensor.cuda()\n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "def optimize_model(replay_memory, model, target_model, optimizer, loss_function):\n",
    "    minibatch = random.sample(replay_memory, min(len(replay_memory), Hyperparameters.MINIBATCH_SIZE))\n",
    "\n",
    "    state_batch = torch.cat(tuple(d[0] for d in minibatch))\n",
    "    action_batch = torch.cat(tuple(d[1] for d in minibatch))\n",
    "    reward_batch = torch.cat(tuple(d[2] for d in minibatch))\n",
    "    new_state_batch = torch.cat(tuple(d[3] for d in minibatch))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        state_batch = state_batch.cuda()\n",
    "        action_batch = action_batch.cuda()\n",
    "        reward_batch = reward_batch.cuda()\n",
    "        new_state_batch = new_state_batch.cuda()\n",
    "\n",
    "    output_1_batch = target_model(new_state_batch)\n",
    "\n",
    "    y_batch = torch.cat(tuple(reward_batch[i] if minibatch[i][4]\n",
    "                              else reward_batch[i] + Hyperparameters.GAMMA * torch.max(output_1_batch[i])\n",
    "                              for i in range(len(minibatch))))\n",
    "\n",
    "    q_value = torch.sum(model(state_batch) * action_batch, dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_batch = y_batch.detach()\n",
    "\n",
    "    loss = loss_function(q_value, y_batch)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def train(model=None, iteration=0):\n",
    "    if not model:\n",
    "      model = NeuralNetwork()\n",
    "      model.apply(init_weights)\n",
    "\n",
    "    target_model = NeuralNetwork()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        target_model = target_model.cuda()\n",
    "\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    start = time.time()\n",
    "\n",
    "    frames_to_skip = 0\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=Hyperparameters.LEARNING_RATE)\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    game_state = GameState(Hyperparameters.SHOW_GAME)\n",
    "    replay_memory = []\n",
    "\n",
    "    action = torch.zeros([Hyperparameters.NUMBER_OF_ACTIONS], dtype=torch.float32)\n",
    "    action[0] = 1\n",
    "    image_data, reward, finished = game_state.frame_step(action)\n",
    "    image_data = image_processing(image_data)\n",
    "    state = torch.cat((image_data, image_data, image_data, image_data)).unsqueeze(0)\n",
    "\n",
    "    epsilon = Hyperparameters.INITIAL_EPSILON\n",
    "\n",
    "    epsilon_decrements = np.concatenate((\n",
    "        np.linspace(Hyperparameters.INITIAL_EPSILON, Hyperparameters.FINAL_EPSILON, min(Hyperparameters.NUMBER_OF_ITERATIONS, Hyperparameters.CUTOFF)),\n",
    "        np.full(max(0, Hyperparameters.NUMBER_OF_ITERATIONS - Hyperparameters.CUTOFF), Hyperparameters.FINAL_EPSILON)\n",
    "    ))\n",
    "\n",
    "    # Metrics for tracking performance\n",
    "    total_reward_per_episode = 0\n",
    "    rewards = []\n",
    "    avg_q_values = []  # To track average Q-value per episode\n",
    "    min_q_values = []  # To track minimum Q-value per episode\n",
    "    max_q_values = []  # To track maximum Q-value per episode\n",
    "    episode_q_values = []  # Temporary list to store Q-values for current episode\n",
    "\n",
    "    while iteration < Hyperparameters.NUMBER_OF_ITERATIONS:\n",
    "        output = model(state)[0]\n",
    "\n",
    "        action = torch.zeros([Hyperparameters.NUMBER_OF_ACTIONS], dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            action = action.cuda()\n",
    "\n",
    "        skipped_frame = False\n",
    "        if frames_to_skip > 0:\n",
    "            skipped_frame = True\n",
    "            frames_to_skip -= 1\n",
    "            action_index = [torch.tensor(0)][0]\n",
    "        else:\n",
    "            if random.random() <= epsilon:\n",
    "                action_index = random.randint(0, Hyperparameters.NUMBER_OF_ACTIONS - 1)\n",
    "            else:\n",
    "                action_index = torch.argmax(output).item()\n",
    "\n",
    "        action[action_index] = 1\n",
    "\n",
    "        if action_index == 1:\n",
    "            frames_to_skip += Hyperparameters.FRAME_SKIP_JUMP\n",
    "        frames_to_skip += Hyperparameters.FRAME_SKIP if not skipped_frame else 0\n",
    "\n",
    "        image_data_1, reward, finished = game_state.frame_step(action)\n",
    "        image_data_1 = image_processing(image_data_1)\n",
    "        new_state = torch.cat((state.squeeze(0)[1:, :, :], image_data_1)).unsqueeze(0)\n",
    "\n",
    "        action = action.unsqueeze(0)\n",
    "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "\n",
    "        reward_float = reward.numpy()[0][0]\n",
    "        if reward_float == -1:\n",
    "            # End of episode: compute Q-value metrics\n",
    "            avg_q_values.append(np.mean(episode_q_values))\n",
    "            min_q_values.append(np.min(episode_q_values))\n",
    "            max_q_values.append(np.max(episode_q_values))\n",
    "            episode_q_values = []  # Reset for the next episode\n",
    "\n",
    "            # Track total rewards per episode\n",
    "            rewards.append(total_reward_per_episode - 1)\n",
    "            total_reward_per_episode = 0\n",
    "        else:\n",
    "            total_reward_per_episode += reward_float\n",
    "\n",
    "        # Append the Q-value of the chosen action to the episode Q-values\n",
    "        episode_q_values.append(output[action_index].item())\n",
    "\n",
    "        # Add transition to replay memory\n",
    "        replay_memory.append((state.cpu(), action.cpu(), reward.cpu(), new_state.cpu(), finished))\n",
    "        if len(replay_memory) > Hyperparameters.REPLAY_MEMORY_SIZE:\n",
    "            replay_memory.pop(0)\n",
    "\n",
    "        epsilon = epsilon_decrements[iteration]\n",
    "\n",
    "        optimize_model(replay_memory, model, target_model, optimizer, loss_function)\n",
    "\n",
    "        state = new_state\n",
    "        iteration += 1\n",
    "\n",
    "        # Update target network periodically\n",
    "        if iteration % Hyperparameters.TARGET_UPDATE_FREQUENCY == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Save the model periodically\n",
    "        if iteration % SAVE_MODEL_EVERY == 0:\n",
    "            torch.save(model, os.path.join(PROJECT_ROOT, \"./models/\", f\"./{Hyperparameters.MODEL_NAME}_\" + str(iteration) + \".pth\"))\n",
    "\n",
    "        # Print metrics every 1000 iterations\n",
    "        if iteration % 1000 == 0:\n",
    "            avg_reward = np.mean(rewards) if rewards else 0\n",
    "            avg_q = np.mean(avg_q_values) if avg_q_values else 0\n",
    "            min_q = np.min(min_q_values) if min_q_values else 0\n",
    "            max_q = np.max(max_q_values) if max_q_values else 0\n",
    "            print(f\"Iteration: {iteration}, Time elapsed: {time.time() - start:.4f}s, epsilon: {epsilon:.4f}, \"\n",
    "                  f\"Avg Rewards: {avg_reward:.4f}, Avg Q: {avg_q:.4f}, Min Q: {min_q:.4f}, Max Q: {max_q:.4f}\")\n",
    "\n",
    "def test(model):\n",
    "    game_state = GameState(Hyperparameters.SHOW_GAME)\n",
    "\n",
    "    action = torch.zeros([Hyperparameters.NUMBER_OF_ACTIONS], dtype=torch.float32)\n",
    "    action[0] = 1\n",
    "    image_data, reward, terminal = game_state.frame_step(action)\n",
    "    image_data = image_processing(image_data)\n",
    "    state = torch.cat((image_data, image_data, image_data, image_data)).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0\n",
    "    points = 0\n",
    "    while True:\n",
    "        output = model(state)[0]\n",
    "\n",
    "        action = torch.zeros([Hyperparameters.NUMBER_OF_ACTIONS], dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            action = action.cuda()\n",
    "\n",
    "        action_index = torch.argmax(output)\n",
    "        if torch.cuda.is_available():\n",
    "            action_index = action_index.cuda()\n",
    "        action[action_index] = 1\n",
    "\n",
    "        image_data_1, reward, terminal = game_state.frame_step(action)\n",
    "        if reward == -1:\n",
    "          print(f\"\\n ------- Finished -------\\nPoints: {points} Total reward: {total_reward}\\n------------------------\")\n",
    "          total_reward = points = 0\n",
    "        elif reward == 1:\n",
    "          points += 1\n",
    "          if points % 10 == 0:\n",
    "              print(f\"Current points: {points}\")\n",
    "        else:\n",
    "          total_reward += reward\n",
    "\n",
    "        image_data_1 = image_processing(image_data_1)\n",
    "        new_state = torch.cat((state.squeeze(0)[1:, :, :], image_data_1)).unsqueeze(0)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "\n",
    "def main(model_path=None, continue_training=False):\n",
    "    cuda_is_available = torch.cuda.is_available()\n",
    "\n",
    "    if model_path:\n",
    "        model = torch.load(model_path,  map_location=torch.device('cpu'), weights_only=False).eval()\n",
    "\n",
    "        if cuda_is_available:\n",
    "            model = model.cuda()\n",
    "\n",
    "        if continue_training:\n",
    "          last_saved_iteration = int(model_path.split(\"_\")[-1].split(\".\")[0])\n",
    "          print(f\"Continue training starting with weights {PRETRAINED_MODEL_WEIGHTS} and iteration {last_saved_iteration}\")\n",
    "          train(model, iteration=last_saved_iteration)\n",
    "        else:\n",
    "          print(f\"Testing model {PRETRAINED_MODEL_WEIGHTS}\")\n",
    "          test(model)\n",
    "\n",
    "    else:\n",
    "        print(f\"Starting to train {Hyperparameters.MODEL_NAME} from scratch. If this is not intended, check your root project path and the .pth file you defined at the start of the script\")\n",
    "        models_folder = os.path.join(PROJECT_ROOT, './models/')\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.mkdir(models_folder)\n",
    "        train()\n",
    "\n",
    "pretrained_model = None if not PRETRAINED_MODEL_WEIGHTS else os.path.join(PROJECT_ROOT, './models/', PRETRAINED_MODEL_WEIGHTS)\n",
    "main(pretrained_model, continue_training=not EVALUATION_MODE)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
